{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import Set, Iterable, NamedTuple, Dict, Tuple\n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str) -> Set[str]:\n",
    "    text = text.lower()\n",
    "    # text = text.replace(\"'\",\"\")\n",
    "    all_words = re.findall(r\"[a-z0-9]+\", text)\n",
    "    return set(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and', 'a', '0', 'of', 'message', 'the', 'contains', 'occurance', 'bitcoin', 'word', 'is', 'this', 'spam'}\n"
     ]
    }
   ],
   "source": [
    "message = \"\"\"This message contains the word \"bitcoin\" and- is a spam message of occurance of 0\"\"\"\n",
    "\n",
    "all_words = tokenize(message)\n",
    "print(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Message(NamedTuple):\n",
    "    text: str\n",
    "    is_spam: bool\n",
    "\n",
    "\n",
    "# Now we have to count, tokenize and label our training data, spam_count = count spam messages, ham_count=no of non spam messages\n",
    "class NaiveBayesClassifier:\n",
    "    def __init__(self, k: float = 0.5) -> None:\n",
    "        self.k = k  # smoothing factor\n",
    "        self.tokens: Set[str] = set()       # vocabulary\n",
    "        self.token_spam_counts: Dict[str, int] = defaultdict(int)\n",
    "        self.token_ham_counts: Dict[str, int] = defaultdict(int)\n",
    "        self.spam_counts = self.ham_counts = 0\n",
    "\n",
    "\n",
    "    # Now we have to train the model by tokenizing each message and for each token we check and increment either spam count or ham count\n",
    "    def train(self, messages: Iterable[Message]) -> None:\n",
    "        for message in messages:\n",
    "            # increment message count\n",
    "            if message.is_spam:\n",
    "                self.spam_counts += 1\n",
    "            else:\n",
    "                self.ham_counts += 1\n",
    "\n",
    "            # check and increment token counts\n",
    "            for token in tokenize(message.text):\n",
    "                self.tokens.add(token)\n",
    "                if message.is_spam:\n",
    "                    self.token_spam_counts[token] += 1\n",
    "                else:\n",
    "                    self.token_ham_counts[token] += 1\n",
    "\n",
    "\n",
    "    # Now ultimately we want to predict P(spam | token) ,as we saw, applying Bayes theoram, we need to to know P(token | s) and we multiply all such individual proabilities, so we have to create a helper function to achieve this\n",
    "    def _probabilities(self, token: str) -> Tuple[float, float]:\n",
    "        \"\"\"Returns P(token| spam) and P(token|ham)\"\"\"\n",
    "        spam = self.token_spam_counts[token]\n",
    "        ham = self.token_ham_counts[token]\n",
    "\n",
    "        p_token_spam = (spam + self.k) / (self.spam_counts + 2 * self.k)        # k is Smooting parameter and applying it to the probabilities\n",
    "        p_token_ham = (ham + self.k) / (self.ham_counts + 2 * self.k)\n",
    "\n",
    "        return p_token_spam, p_token_ham\n",
    "    \n",
    "\n",
    "    def predict(self, text: str) -> float:\n",
    "        text_tokens = tokenize(text)\n",
    "        log_prob_if_spam = log_prob_if_ham = 0\n",
    "\n",
    "        # iterate through each word in vocabulary\n",
    "        for token in self.tokens:\n",
    "            prob_if_spam, prob_if_ham = self._probabilities(token)\n",
    "\n",
    "            # if *token* appears in message, add the log probability of seeing it\n",
    "            if token in text_tokens:\n",
    "                log_prob_if_spam += math.log(prob_if_spam)\n",
    "                log_prob_if_ham += math.log(prob_if_ham)\n",
    "\n",
    "            # otherwise, add log probability of not seeing it, which is log(1-probability of seeing it)\n",
    "            else:\n",
    "                log_prob_if_spam += math.log(1.0 - prob_if_spam)\n",
    "                log_prob_if_ham += math.log(1.0 - prob_if_ham)\n",
    "\n",
    "        prob_if_spam = math.exp(log_prob_if_spam)       # P(Xi = wi|S)\n",
    "        prob_if_ham = math.exp(log_prob_if_ham)       # P(Xi = wi|~S)\n",
    "\n",
    "        total_probability = prob_if_spam / (prob_if_spam + prob_if_ham)       # P(S | Xi = W)\n",
    "        return total_probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing our Naive Bayes classifying model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_messages = [\n",
    "    Message(\"happy birthday. hope to see you next week\", is_spam=False),\n",
    "    Message(\"spam rules\", is_spam=True),\n",
    "    Message(\"be alert of spam messages\", is_spam=False),\n",
    "    Message(\"Claim your reward\", is_spam=True),\n",
    "    Message(\"Buy our hottest product\", is_spam=True),\n",
    "    Message(\"hurry offer exrires now\", is_spam=True),\n",
    "    Message(\"where is the report\", is_spam=False),\n",
    "    Message(\"you are invited to the event\", is_spam=False),\n",
    "    Message(\"your computer has a virus\", is_spam=True),\n",
    "    Message(\"we have detected a fault in your previous transaction\", is_spam=True),\n",
    "    Message(\"join our course to quickly make money from trading\", is_spam=True),\n",
    "    Message(\"if you buy our mattress, you will get a free coupon for the next purchase\", is_spam=True),\n",
    "]\n",
    "\n",
    "model = NaiveBayesClassifier(k=0.5)\n",
    "model.train(training_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words extracted (Tokens) from the messages are: {'week', 'a', 'computer', 'detected', 'hottest', 'you', 'see', 'happy', 'reward', 'your', 'now', 'if', 'has', 'quickly', 'in', 'purchase', 'hurry', 'mattress', 'where', 'fault', 'virus', 'claim', 'offer', 'have', 'our', 'the', 'we', 'exrires', 'trading', 'hope', 'transaction', 'event', 'money', 'are', 'free', 'report', 'invited', 'messages', 'coupon', 'alert', 'previous', 'join', 'course', 'buy', 'to', 'rules', 'be', 'get', 'for', 'will', 'birthday', 'from', 'next', 'product', 'of', 'is', 'make', 'spam'}\n",
      "No. of spam messages are: 8\n",
      "No. of ham messages are: 4\n",
      "Word count in spam messages: {'rules': 1, 'spam': 1, 'reward': 1, 'your': 3, 'claim': 1, 'buy': 2, 'product': 1, 'hottest': 1, 'our': 3, 'offer': 1, 'hurry': 1, 'exrires': 1, 'now': 1, 'a': 3, 'virus': 1, 'has': 1, 'computer': 1, 'detected': 1, 'fault': 1, 'have': 1, 'we': 1, 'previous': 1, 'transaction': 1, 'in': 1, 'from': 1, 'join': 1, 'make': 1, 'quickly': 1, 'course': 1, 'trading': 1, 'to': 1, 'money': 1, 'get': 1, 'mattress': 1, 'free': 1, 'for': 1, 'you': 1, 'will': 1, 'if': 1, 'coupon': 1, 'the': 1, 'next': 1, 'purchase': 1}\n",
      "Word count in ham messages: {'week': 1, 'you': 2, 'see': 1, 'happy': 1, 'birthday': 1, 'next': 1, 'hope': 1, 'to': 2, 'messages': 1, 'alert': 1, 'of': 1, 'be': 1, 'spam': 1, 'is': 1, 'the': 2, 'where': 1, 'report': 1, 'are': 1, 'invited': 1, 'event': 1}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Words extracted (Tokens) from the messages are: {model.tokens}\")\n",
    "print(f\"No. of spam messages are: {model.spam_counts}\")\n",
    "print(f\"No. of ham messages are: {model.ham_counts}\")\n",
    "print(f\"Word count in spam messages: {dict(model.token_spam_counts)}\")\n",
    "print(f\"Word count in ham messages: {dict(model.token_ham_counts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets see how the prediction works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_message = \"you are invited to our spam detection course\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the probability of spam and ham using our **model**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The given message has a chance of 3.8% being a spam message based on training data above\n"
     ]
    }
   ],
   "source": [
    "percentile_spam = round(100 * (model.predict(test_message)), 1)\n",
    "\n",
    "print(f\"The given message has a chance of {percentile_spam}% being a spam message based on training data above\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
