{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate-Linear-Regression-From-Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model:**\n",
    "$$\n",
    "y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_k x_{ik} + \\epsilon\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Features (independent variable) are linearly independent and it has a linear relationship with the dependent variable\n",
    "\n",
    "- Features are all uncorrelated with epslon (error of model)\n",
    "\n",
    "- For the sake of implementition, we assume the value of epsilon is zero on average (hence ignoring its effects)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implemented Model**\n",
    "\n",
    "```python\n",
    "y_i = beta_0 + beta_1 * x_i_1 + beta_2 * x_i_2 + ..........+ beta_k * x_i_k + epsilon\n",
    "\n",
    "Beta = [b_0, b_1, b_2, ....... b_k]\n",
    "\n",
    "X_i = [x_i_1, x_i_2, ........ x_i_k]\n",
    "\n",
    "Y_i = Beta * X_i + beta_0 + epsilon   # dot product of Beta_vector and x_i\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from typing import List\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type definitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vector = np.ndarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Decsent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Gradient Decsent to find the minimum value of the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descent_one_step(v: Vector, gradient: Vector, learning_rate: float) -> Vector:\n",
    "    \"\"\"Starts from v and moves a step units in oppsite direction of gradient\"\"\"\n",
    "    assert len(v) == len(gradient), \"vector and its gradient are of different lengths\"\n",
    " \n",
    "    step = (-learning_rate) * gradient\n",
    "    return v + step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x: Vector, beta: Vector) -> float:\n",
    "    \"\"\"Returns the predicted 'y' (target) value given 'x' (features) and co-efficients\"\"\"\n",
    "    return x.dot(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss function (error function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(x: Vector, y: float, beta: Vector) -> float:\n",
    "    \"\"\"Returns the error of predicted value from the actual value (True value)\"\"\"\n",
    "    return predict(x, beta) - y\n",
    "\n",
    "\n",
    "def squared_error(x: Vector, y: float, beta: Vector) -> float:\n",
    "    \"\"\"Returns the squared value of error\"\"\"\n",
    "    return error(x, y, beta) ** 2\n",
    "\n",
    "\n",
    "def squarred_error_gradient(x: Vector, y: float, beta: Vector) -> Vector:\n",
    "    \"\"\"Returns the gradient of error function\"\"\"\n",
    "    err = error(x, y, beta)\n",
    "    return [2 * err] + [2 * err * x_i for x_i in x[1:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stochastic\n",
    "\n",
    "def least_squares_fit(\n",
    "    xs: Vector[Vector],\n",
    "    ys: Vector,\n",
    "    learning_rate: float = 0.001,\n",
    "    num_steps: int = 1000,\n",
    "    batch_pct: int = 1,\n",
    ") -> Vector:\n",
    "    \"\"\"Computes the parameters (Beta) that minimizes sum of squared errors using gradient descent\"\"\"\n",
    "\n",
    "    random.seed(0)\n",
    "    sample_size = len(xs)\n",
    "    features_num = len(xs[0])\n",
    "    batch_size = int(sample_size * batch_pct)\n",
    "\n",
    "    # start with a random starting point for the parameters of the model (Beta)\n",
    "    Beta = [random.random() for _ in range(features_num)]\n",
    "\n",
    "    with tqdm.trange(num_steps, desc=\"Ordinary Least Squares Fit\") as t:\n",
    "        for epoch in t:\n",
    "            index = random.randint(0, sample_size - 1 - batch_size) if batch_size != sample_size else 0\n",
    "            batch_xs = xs[index : index + batch_size]\n",
    "            batch_ys = ys[index : index + batch_size]\n",
    "\n",
    "            err_func_grads = [\n",
    "                squarred_error_gradient(x, y, Beta) for x, y in zip(batch_xs, batch_ys)\n",
    "            ]\n",
    "            gradient = np.mean(err_func_grads, axis=0)\n",
    "\n",
    "            Beta = descent_one_step(Beta, gradient, learning_rate)\n",
    "\n",
    "    return Beta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
