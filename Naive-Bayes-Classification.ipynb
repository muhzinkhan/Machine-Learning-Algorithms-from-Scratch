{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S: spam\n",
    "\n",
    "B: containing the word \"Bitcoin\"\n",
    "\n",
    "a message is spam given on condition that it contains \"Bitcoin\":\n",
    "\n",
    "```\n",
    "P(S|B) = [P(B|S) * P(S)] / [P(B|S) * P(S) + P(B|~S) * P(~S)]\n",
    "```\n",
    "\n",
    "Numerator: Probability that a message is spam and it contains \"Bitcoin\"\n",
    "\n",
    "Denominator: Probability that a message contains \"Bitcoin\"\n",
    "\n",
    "```\n",
    "P(S|B) = P(B|S) / [P(B|S) + P(B|~S)]\n",
    "```\n",
    "\n",
    "Eg:-\n",
    "\n",
    "- our data: 50% of spam messages contains bitcoin\n",
    "\n",
    "     only 1% of non-spam messages contains the word Bitcoin\n",
    "\n",
    "     ```\n",
    "     P(B|S) = 0.5\n",
    "     \n",
    "     P(B|~S) = 0.01\n",
    "     \n",
    "     P(S|B) = 0.5 / (0.5+.01) = 0.98\n",
    "     ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*A more better spam filter*:\n",
    "\n",
    "We decided to increase the reach of our spam filter by adding more words to the list of words that may leads to spam.\n",
    "\n",
    "supose w = [w1, w2, w3, ....wn] is the vocabulary for spam filter\n",
    "\n",
    "suppose Xi is the event \"A message contains the word wi\"\n",
    "\n",
    "S = event that a message is spam\n",
    "\n",
    "~S = event that a message is not spam\n",
    "\n",
    "`P(Xi | S)` = is the probability that a message contains the words wi and itsa spam message\n",
    "\n",
    "`P(Xi | ~S)` = is the probability that a message contains the word wi but its not a spam message\n",
    "\n",
    "**Assumption**\n",
    "\n",
    "*The key idea to Naive bayes is making a \"Naive\" assumption that the presence or absence of each word in vocabulary are independent of each other, conditional on a message being spam or not.*\n",
    "\n",
    "Eg:-\n",
    "\n",
    "It Assumes knowing a certain spam message contains the word 'Bitcoin' gives no information about whether the same spam message contains the word 'Rolex' or not\n",
    "\n",
    "Applying this assumption on the probability model:\n",
    "\n",
    "xi = partial probability, ie, occurrence of multiple words (eg: w3, w5, ... wi)\n",
    "\n",
    "`P(x1=X1, x2=X2, ..... xn=Xn | S) = P(x1=X1 | S) * P(x2=X2 | S) * .... * P(xn=Xn | S)`\n",
    "\n",
    "In another way:\n",
    "\n",
    "`P(S | x=X) = P(x=X | S) / [P(x=X | S) + P(x=X | ~S)]`\n",
    "\n",
    "Naive Bayes assumption allows us to compute each of the probabilities in the right by simply multiplying together the individual probabilities for each vocabulary word.\n",
    "\n",
    "In real cases to avoid \"*underflow*\":- in which computers deal with real small floating probability estimates that are very close to zero, we find exponential of log\n",
    "\n",
    "since:\n",
    "\n",
    "$log(a.b) = log(a) + log(b)$\n",
    "\n",
    "$exp(log(x)) = x$\n",
    "\n",
    "then:\n",
    "\n",
    "means an estimate `p1 * p2 * p3 * ... *pn = exp(1og(p1) + 1og(p2) ... 1og(pn))`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose \"data\" is a word that never appeard in a spam message.\n",
    "\n",
    "`P(Xi=\"data\"|S) = 0`, So we use a \"*pseudocount*\" k, which modifies the probability calculation as:\n",
    "\n",
    "`P(Xi=wi | S) = (k + number of spam messages containing wi) / (2*K + number of spams)`\n",
    "\n",
    "- Eg:- \n",
    "\n",
    "    \"authentic\" appears in 0 out of 98 spam messages.\n",
    "    \n",
    "    `P(Xi=\"authentic\"|S) = 0`\n",
    "    \n",
    "    So we add 1 more count as a pseudo count to the probability and get: \n",
    "    \n",
    "    `P(X=\"authentic\"|S) = [1+0 / (2 * 1 + 98)] = 1/100` to avoid zero probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ALGORITHM**\n",
    "\n",
    "1. classifier\n",
    "2. tokenizer\n",
    "3. clean\n",
    "\n",
    "\n",
    "Example message:\n",
    "\n",
    "\"\"\"\n",
    "*This message contains the word \"bitcoin\" and- is a spam message of occurance of 0*\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import Set, Iterable, NamedTuple, Dict, Tuple\n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str) -> Set[str]:\n",
    "    text = text.lower()\n",
    "    # text = text.replace(\"'\",\"\")\n",
    "    all_words = re.findall(r\"[a-z0-9]+\", text)\n",
    "    return set(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this', 'the', 'bitcoin', 'and', 'is', 'a', 'occurance', 'of', 'word', '0', 'message', 'spam', 'contains'}\n"
     ]
    }
   ],
   "source": [
    "message = \"\"\"This message contains the word \"bitcoin\" and- is a spam message of occurance of 0\"\"\"\n",
    "\n",
    "all_words = tokenize(message)\n",
    "print(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Message(NamedTuple):\n",
    "    text: str\n",
    "    is_spam: bool\n",
    "\n",
    "\n",
    "# Now we have to count, tokenize and label our training data, spam_count = count spam messages, ham_count=no of non spam messages\n",
    "class NaiveBayesClassifier:\n",
    "    def __init__(self, k: float = 0.5) -> None:\n",
    "        self.k = k  # smoothing factor\n",
    "        self.tokens: Set[str] = set()       # vocabulary\n",
    "        self.token_spam_counts: Dict[str, int] = defaultdict(int)\n",
    "        self.token_ham_counts: Dict[str, int] = defaultdict(int)\n",
    "        self.spam_counts = self.ham_counts = 0\n",
    "\n",
    "\n",
    "    # Now we have to train the model by tokenizing each message and for each token we check and increment either spam count or ham count\n",
    "    def train(self, messages: Iterable[Message]) -> None:\n",
    "        for message in messages:\n",
    "            # increment message count\n",
    "            if message.is_spam:\n",
    "                self.spam_counts += 1\n",
    "            else:\n",
    "                self.ham_counts += 1\n",
    "\n",
    "            # check and increment token counts\n",
    "            for token in tokenize(message.text):\n",
    "                self.tokens.add(token)\n",
    "                if message.is_spam:\n",
    "                    self.token_spam_counts[token] += 1\n",
    "                else:\n",
    "                    self.token_ham_counts[token] += 1\n",
    "\n",
    "\n",
    "    # Now ultimately we want to predict P(spam | token) ,as we saw, applying Bayes theoram, we need to to know P(token | s) and we multiply all such individual proabilities, so we have to create a helper function to achieve this\n",
    "    def _probabilities(self, token: str) -> Tuple[float, float]:\n",
    "        \"\"\"Returns P(token| spam) and P(token|ham)\"\"\"\n",
    "        spam = self.token_spam_counts[token]\n",
    "        ham = self.token_ham_counts[token]\n",
    "\n",
    "        p_token_spam = (spam + self.k) / (self.spam_counts + 2 * self.k)        # k is Smooting parameter and applying it to the probabilities\n",
    "        p_token_ham = (ham + self.k) / (self.ham_counts + 2 * self.k)\n",
    "\n",
    "        return p_token_spam, p_token_ham\n",
    "    \n",
    "\n",
    "    def predict(self, text: str) -> float:\n",
    "        text_tokens = tokenize(text)\n",
    "        log_prob_if_spam = log_prob_if_ham = 0\n",
    "\n",
    "        # iterate through each word in vocabulary\n",
    "        for token in self.tokens:\n",
    "            prob_if_spam, prob_if_ham = self._probabilities(token)\n",
    "\n",
    "            # if *token* appears in message, add the log probability of seeing it\n",
    "            if token in text_tokens:\n",
    "                log_prob_if_spam += math.log(prob_if_spam)\n",
    "                log_prob_if_ham += math.log(prob_if_ham)\n",
    "\n",
    "            # otherwise, add log probability of not seeing it, which is log(1-probability of seeing it)\n",
    "            else:\n",
    "                log_prob_if_spam += math.log(1.0 - prob_if_spam)\n",
    "                log_prob_if_ham += math.log(1.0 - prob_if_ham)\n",
    "\n",
    "        prob_if_spam = math.exp(log_prob_if_spam)       # P(Xi = wi|S)\n",
    "        prob_if_ham = math.exp(log_prob_if_ham)       # P(Xi = wi|~S)\n",
    "\n",
    "        total_probability = prob_if_spam / (prob_if_spam + prob_if_ham)       # P(S | Xi = W)\n",
    "        return total_probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing our Naive Bayes classifying model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets test our model by writing a unit test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages = [\n",
    "#     Message(\"spam rules\", is_spam=True),\n",
    "#     Message(\"spam rules\", is_spam=False),\n",
    "#     Message(\"ham message\", is_spam=False),\n",
    "# ]\n",
    "\n",
    "messages = [\n",
    "    Message(\"spam rules\", is_spam=True),\n",
    "    Message(\"not ham rules\", is_spam=False),\n",
    "    Message(\"not a spam message\", is_spam=False),\n",
    "]\n",
    "\n",
    "model = NaiveBayesClassifier(k=0.5)\n",
    "model.train(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words extracted (Tokens) from the messages are: {'a', 'ham', 'rules', 'message', 'not', 'spam'}\n",
      "No. of spam messages are: 1\n",
      "No. of ham messages are: 2\n",
      "Word count in spam messages: {'spam': 1, 'rules': 1}\n",
      "Word count in ham messages: {'ham': 1, 'not': 2, 'rules': 1, 'a': 1, 'message': 1, 'spam': 1}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Words extracted (Tokens) from the messages are: {model.tokens}\")\n",
    "print(f\"No. of spam messages are: {model.spam_counts}\")\n",
    "print(f\"No. of ham messages are: {model.ham_counts}\")\n",
    "print(f\"Word count in spam messages: {dict(model.token_spam_counts)}\")\n",
    "print(f\"Word count in ham messages: {dict(model.token_ham_counts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets see how the prediction works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"hello spam\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the probability of spam and ham using **bayes theoram**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes Theorem: of the 1st testing message\n",
    "\n",
    "```\n",
    "P(S|B) = [P(B|S) * P(S)] / [P(B|S) * P(S) + P(B|~S) * P(~S)]\n",
    "\n",
    "since P(S: Prob of spam), P(~S: Prob of ham) = 0.5\n",
    "\n",
    "P(S|B) = P(B|S) / [P(B|S) + P(B|~S)] --> Objective\n",
    "\n",
    "\n",
    "P(Xi = 'hello'|S) = 0/1 = 0+1/(1 + 2 * 1) = 0.33             (k=1)\n",
    "\n",
    "P(Xi = 'spam'|S) = 1/1 = (1+1)/(1+ 2 * 1) = 0.66             (No.of messages containing word 'spam' / No. of Spams)\n",
    "\n",
    "\n",
    "P(Xi = 'hello'|~S) = 0/2 = 0+1/(2 + 2 * 1) = 0.25\n",
    "\n",
    "P(Xi = 'spam'|~S) = 2/2 = (2+1)/(2 + 2 * 1) = 0.75\n",
    "\n",
    "\n",
    "P(Xi='hello' AND Xi = 'spam'|S) = P(Xi = 'hello'|S) * P(Xi = 'spam'|S)          (with the naive assumption that: P(Xi = 'hello') & P(Xi = 'spam') are independent)\n",
    "\n",
    "P(Xi='hello' AND Xi = 'spam'|S) = 0.33 * 0.66 = 0.218\n",
    "\n",
    "P(Xi='hello' AND Xi = 'spam'|~S) = P(Xi = 'hello'|~S) * P(Xi = 'spam'|~S)\n",
    "\n",
    "P(Xi='hello' AND Xi = 'spam'|~S) = 0.25 * 0.75 = 0.187\n",
    "\n",
    "```\n",
    "\n",
    "Finally:\n",
    "\n",
    "`P(S | prob of our vocab occouring in our message) = P(S | Xi='hello' AND Xi = 'spam') = 0.218/(0.218 + 0.187) = 0.538`\n",
    "\n",
    "This is 53.8% of message being a spam (Our Model spits out: 62.8%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes Theorem: of the 2nd testing message <--------------------------------\n",
    "\n",
    "```\n",
    "P(S|B) = [P(B|S) * P(S)] / [P(B|S) * P(S) + P(B|~S) * P(~S)]\n",
    "\n",
    "since P(S), P(~S) = 0.5\n",
    "\n",
    "P(S|B) = P(B|S) / [P(B|S) + P(B|~S)] --> Objective\n",
    "\n",
    "\n",
    "P(Xi = 'hello'|S) = 0/1 = 0+1/(1 + 2 * 1) = 0.33             (k=1)\n",
    "\n",
    "P(Xi = 'spam'|S) = 1/1 = (1+1)/(1 + 2 * 1) = 0.66\n",
    "\n",
    "\n",
    "P(Xi = 'hello'|~S) = 0/2 = 0+1/(2 + 2 * 1) = 0.25\n",
    "\n",
    "P(Xi = 'spam'|~S) = 1/2 = (1+1)/(2 + 2 * 1) = 0.5\n",
    "\n",
    "\n",
    "P(Xi='hello' AND Xi = 'spam'|S) = P(Xi = 'hello'|S) * P(Xi = 'spam'|S)          (with the naive assumption that: P(Xi = 'hello') & P(Xi = 'spam') are independent)\n",
    "\n",
    "P(Xi='hello' AND Xi = 'spam'|S) = 0.33 * 0.66 = 0.218\n",
    "\n",
    "P(Xi='hello' AND Xi = 'spam'|~S) = P(Xi = 'hello'|~S) * P(Xi = 'spam'|~S)\n",
    "\n",
    "P(Xi='hello' AND Xi = 'spam'|~S) = 0.25 * 0.5 = 0.125\n",
    "\n",
    "```\n",
    "\n",
    "Finally:\n",
    "\n",
    "`P(S | prob of input message words(altogether) occouring in our vocabulary) = P(S | Xi='hello' AND Xi = 'spam') = 0.218/(0.218 + 0.125) = 0.635`\n",
    "<!-- `P(S | prob of our vocab occouring in our message) = P(S | Xi='hello' AND Xi = 'spam') = 0.218/(0.218 + 0.125) = 0.635` -->\n",
    "\n",
    "This is 63.5% of message being a spam (Our Model spits out: 91.9%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the probability of spam and ham using our **model**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The given message has a chance of 91.9% being a spam message based on training data above\n"
     ]
    }
   ],
   "source": [
    "percentile_spam = round(100 * (model.predict(text)), 1)\n",
    "\n",
    "print(f\"The given message has a chance of {percentile_spam}% being a spam message based on training data above\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
