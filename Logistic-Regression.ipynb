{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from typing import List, Tuple, TypeVar\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type definitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vector = np.ndarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model:**\n",
    "$$\n",
    "log(\\frac{Prob(classification)}{1-Prob(classification)}) = y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_k x_{ik} + \\epsilon\n",
    "\\\\\n",
    "probability(classification) = \\frac{e^{y_i}}{1+e^{y_i}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Features (independent variable) are linearly independent and it has a linear relationship with the dependent variable\n",
    "\n",
    "- Features are all uncorrelated with epslon (error of model)\n",
    "\n",
    "- For the sake of implementition, we assume the value of epsilon is zero on average (hence ignoring its effects)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implemented Model**\n",
    "\n",
    "```python\n",
    "y_i = beta_0 + beta_1 * x_i_1 + beta_2 * x_i_2 + ..........+ beta_k * x_i_k + epsilon\n",
    "\n",
    "Beta = [b_0, b_1, b_2, ....... b_k]\n",
    "\n",
    "X_i = [x_i_1, x_i_2, ........ x_i_k]\n",
    "\n",
    "Y_i = Beta * X_i + beta_0 + epsilon   # dot product of Beta_vector and x_i\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://datatab.net/assets/tutorial/Logistic-function.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOGISTIC FUNCTION: Modified to fix floating point overflow\n",
    "\n",
    "def logistic(x: float) -> float:\n",
    "    \"\"\"Applying logistic function on x\"\"\"\n",
    "    if x < -709:\n",
    "        return 0.0\n",
    "    elif x > 709:\n",
    "        return 1.0\n",
    "    return 1.0 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As defined in the **logistic function** (Sigmoid function) is a type of *logarithmic function*. As input gets large and positive, it gets closer and closer to 1.as input gets larger and negative,it gets closer to zero. And its derivative could be calculated by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using this to fit the model:\n",
    "`y_i = F( x_i * beta) + epslon`\n",
    "\n",
    "F = Logistic function\n",
    "\n",
    "recall we did in linear regression,using squared error function we calculated beta that minimizes the error or maximizes the likelyhood of data. Here we have to use gradient descent to maximixe the likelyhood of data.\n",
    "\n",
    "intrepeting the model: y_i = F(x_i * beta)\n",
    "\n",
    "given some beta, each y_i should equal to one with a probability F(x_i*beat) and equal to zero with probability 1-F(x_i * beta)\n",
    "\n",
    "now, the PDF for y_i is:\n",
    "\n",
    "$P(y_i|x_i, beta) = (F(x_i*beta)**y_i) • (1-F(x_i*beta))**(1-y_i)$  ---------------> eq(1)\n",
    "\n",
    "since y_i = 0:\n",
    "1 - F(x_i * beta)\n",
    "\n",
    "since y_i = 1:\n",
    "F(x_i * beta)\n",
    "\n",
    "\n",
    "applying log on eq(1)\n",
    "\n",
    "$logL(beta|x_i, y_i) = y_i • log(F(x_i*beta)) + (1 - y_i) • log(1 - F(x_i*beta))$\n",
    "\n",
    "now log is a strictly increasing function, any beta maximizes the likelihood of data also maximizes the likelyhood of log likelyhood. since gradient descent minimizes things,we work with negative log likelyhood, because maximizing likelyhood is same as minimizing its negative.\n",
    "\n",
    "\"Log likelyhood function\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _negative_log_likelihood(x: Vector, y: float, beta: Vector) -> float:\n",
    "    \"\"\"calculates the negative log likelihood for one data point\"\"\"\n",
    "    if y == 1:\n",
    "        return -np.log(logistic(x.dot(beta)))\n",
    "    else:\n",
    "        return -np.log(1 - logistic(x.dot(beta)))\n",
    "\n",
    "\n",
    "# assuming different observations (data points) are independent from each other\n",
    "# overoll log likelihood is the sum of individual log likelihoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_log_likelihood(xs: List[Vector], ys: Vector, beta: Vector) -> float:\n",
    "    return np.array([_negative_log_likelihood(x, y, beta) for x, y in zip(xs, ys)]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have to find the gradient\n",
    "def _negative_log_partial_j(x: Vector, y: float, beta: Vector, j: int) -> float:\n",
    "    \"\"\"calculates the j-th partial derivative at one data point,here j is the index of parameters in a single data point\"\"\"\n",
    "    return -(y - logistic(x.dot(beta))) * x[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _negative_log_gradient(x: Vector, y: float, beta: Vector) -> Vector:\n",
    "    \"\"\"gradient for one data point\"\"\"\n",
    "    return np.array([_negative_log_partial_j(x, y, beta, j) for j in range(len(beta))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_log_gradient(xs: List[Vector], ys: Vector, beta: Vector) -> Vector:\n",
    "    return np.array([_negative_log_gradient(x, y, beta) for x, y in zip(xs, ys)]).sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rescaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In ML,when dimensions arent comparable with each other,we may have to \"rescale\" our data so that each diamention has mean zero and standerd deviation 1. This effectivly get rid of units converting each dimension to standard deviations from mean. so to begin with we need to calculate mean and std deviation too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pre-reqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_error(xs_error: Vector) -> Vector:\n",
    "    \"\"\"Returns the squared value of each element in the list of error\"\"\"\n",
    "    return np.array([x_i**2 for x_i in xs_error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_from_mean(xs: Vector) -> Vector:\n",
    "    x_bar = xs.mean()\n",
    "    return np.array([x - x_bar for x in xs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variance(xs: Vector) -> float:\n",
    "    \"\"\"accepts a list of data points and returns its variance from mean\"\"\"\n",
    "    n = len(xs)\n",
    "    xs_error = error_from_mean(xs)\n",
    "    xs_error_squared = squared_error(xs_error)\n",
    "    return sum(xs_error_squared) / (n - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_deviation(xs: Vector) -> float:\n",
    "    \"\"\"accepts a list and returns its Standerd deviation from mean\"\"\"\n",
    "    return math.sqrt(variance(xs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scale_parameters(data: List[Vector]) -> Tuple[Vector, Vector]:\n",
    "    \"\"\"returns the mean and std deviation of each position\"\"\"\n",
    "    dim = len(data[0])\n",
    "    means = np.mean(data, axis=0)\n",
    "    stdevs = np.array([np.array([vector[i] for vector in data]).std() for i in range(dim)])\n",
    "    return means, stdevs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(data: List[Vector]) -> List[Vector]:\n",
    "    \"\"\"rescale the input data so that each position has a mean of 0 and std dev=1\"\"\"\n",
    "    dim = len(data[0])\n",
    "    means, stdevs = calculate_scale_parameters(data)\n",
    "    rescaled = [v[:] for v in data]  # make a copy of each vector\n",
    "    for v in rescaled:\n",
    "        for i in range(dim):\n",
    "            if stdevs[i] > 0:\n",
    "                v[i] = (v[i] - means[i]) / stdevs[i]\n",
    "\n",
    "    return rescaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descent_one_step(v: Vector, gradient: Vector, learning_rate: float) -> Vector:\n",
    "    \"\"\"Starts from v and moves a step units in oppsite direction of gradient\"\"\"\n",
    "    assert len(v) == len(gradient), \"The vector and its gradient are of different lengths\"\n",
    " \n",
    "    step = (-learning_rate) * gradient\n",
    "    return v + step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generic type to represnt a data point\n",
    "X = TypeVar(\"X\")\n",
    "Y = TypeVar(\"Y\")\n",
    "\n",
    "\n",
    "def split_data(data: List[X], prob: float, seed:int = None) -> Tuple[List[X], List[X]]:\n",
    "    \"\"\"Split the given data into fractions of prob('prob' is fraction of training data set) given [[prob, 1-prob]]\"\"\"\n",
    "    data = data.copy()\n",
    "    random.seed(seed)\n",
    "    random.shuffle(data)\n",
    "    cut = int(len(data) * prob)\n",
    "    return data[:cut], data[cut:]\n",
    "\n",
    "\n",
    "def train_test_split(\n",
    "    xs: List[X], ys: List[Y], test_pct: float, seed:int=None\n",
    ") -> Tuple[List[X], List[X], List[Y], List[Y]]:\n",
    "    \"\"\"Split them by using indeices\"\"\"\n",
    "\n",
    "    idxs = [i for i in range(len(xs))]\n",
    "    train_idxs, test_idxs = split_data(idxs, 1 - test_pct, seed=seed)\n",
    "\n",
    "    return (\n",
    "        np.array([xs[i] for i in train_idxs]),      # x train\n",
    "        np.array([xs[i] for i in test_idxs]),       # x test,\n",
    "        np.array([ys[i] for i in train_idxs]),      # y train, \n",
    "        np.array([ys[i] for i in test_idxs]),       # y test\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting model into data: (*logistic regression*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fitting the Logistic Regression model\n",
    "\n",
    "# random.seed(0)\n",
    "# xs = features.values\n",
    "# rescaled_xs = scale(xs)\n",
    "# ys = target['Purchased'].values\n",
    "# x_train, x_test, y_train, y_test = train_test_split(rescaled_xs, ys, 0.33)\n",
    "\n",
    "# learning_rate = 0.001\n",
    "\n",
    "\n",
    "# # Pick a random starting point\n",
    "# beta = np.array([random.random() for _ in range(len(xs[0]))])\n",
    "\n",
    "# with tqdm.trange(5000) as t:\n",
    "#     for epoch in t:\n",
    "#         gradient = negative_log_gradient(x_train, y_train, beta)\n",
    "#         beta = descent_one_step(beta, gradient, learning_rate)\n",
    "#         loss = negative_log_likelihood(x_train, y_train, beta)\n",
    "#         t.set_description(f\"loss = {loss:.5f}\")\n",
    "\n",
    "# print(beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(x_train: List[X], y_train:List[Y], learning_rate:float = 0.001, max_iter:int = 5000):\n",
    "    \"\"\"Returns the Beta for with the log-likelihood is maximum. Using gradient descent to minimize the negative of maximum log likelihood function\"\"\"\n",
    "    \n",
    "    # Pick a random starting point\n",
    "    beta = np.array([random.random() for _ in range(len(x_train[0]))])\n",
    "\n",
    "    with tqdm.trange(max_iter) as t:\n",
    "        for epoch in t:\n",
    "            gradient = negative_log_gradient(x_train, y_train, beta)\n",
    "            beta = descent_one_step(beta, gradient, learning_rate)\n",
    "            loss = negative_log_likelihood(x_train, y_train, beta)\n",
    "            t.set_description(f\"loss = {loss:.5f}\")\n",
    "\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing with a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! kaggle datasets download -d dragonheir/logistic-regression\n",
    "# ! unzip -o \"logistic-regression.zip\"\n",
    "# ! rm \"logistic-regression.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/Social_Network_Ads.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Purchased</th>\n",
       "      <th>is_male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15624510</td>\n",
       "      <td>Male</td>\n",
       "      <td>19</td>\n",
       "      <td>19000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15810944</td>\n",
       "      <td>Male</td>\n",
       "      <td>35</td>\n",
       "      <td>20000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15668575</td>\n",
       "      <td>Female</td>\n",
       "      <td>26</td>\n",
       "      <td>43000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15603246</td>\n",
       "      <td>Female</td>\n",
       "      <td>27</td>\n",
       "      <td>57000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15804002</td>\n",
       "      <td>Male</td>\n",
       "      <td>19</td>\n",
       "      <td>76000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>15691863</td>\n",
       "      <td>Female</td>\n",
       "      <td>46</td>\n",
       "      <td>41000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>15706071</td>\n",
       "      <td>Male</td>\n",
       "      <td>51</td>\n",
       "      <td>23000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>15654296</td>\n",
       "      <td>Female</td>\n",
       "      <td>50</td>\n",
       "      <td>20000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>15755018</td>\n",
       "      <td>Male</td>\n",
       "      <td>36</td>\n",
       "      <td>33000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>15594041</td>\n",
       "      <td>Female</td>\n",
       "      <td>49</td>\n",
       "      <td>36000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      User ID  Gender  Age  EstimatedSalary  Purchased  is_male\n",
       "0    15624510    Male   19            19000          0        1\n",
       "1    15810944    Male   35            20000          0        1\n",
       "2    15668575  Female   26            43000          0        0\n",
       "3    15603246  Female   27            57000          0        0\n",
       "4    15804002    Male   19            76000          0        1\n",
       "..        ...     ...  ...              ...        ...      ...\n",
       "395  15691863  Female   46            41000          1        0\n",
       "396  15706071    Male   51            23000          1        1\n",
       "397  15654296  Female   50            20000          1        0\n",
       "398  15755018    Male   36            33000          0        1\n",
       "399  15594041  Female   49            36000          1        0\n",
       "\n",
       "[400 rows x 6 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['is_male'] = df['Gender'].map({'Male': 1, 'Female': 0})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df[[\"Age\", \"EstimatedSalary\", \"is_male\"]]\n",
    "target = df[[\"Purchased\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>is_male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>19000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35</td>\n",
       "      <td>20000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26</td>\n",
       "      <td>43000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27</td>\n",
       "      <td>57000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19</td>\n",
       "      <td>76000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>46</td>\n",
       "      <td>41000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>51</td>\n",
       "      <td>23000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>50</td>\n",
       "      <td>20000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>36</td>\n",
       "      <td>33000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>49</td>\n",
       "      <td>36000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Age  EstimatedSalary  is_male\n",
       "0     19            19000        1\n",
       "1     35            20000        1\n",
       "2     26            43000        0\n",
       "3     27            57000        0\n",
       "4     19            76000        1\n",
       "..   ...              ...      ...\n",
       "395   46            41000        0\n",
       "396   51            23000        1\n",
       "397   50            20000        0\n",
       "398   36            33000        1\n",
       "399   49            36000        0\n",
       "\n",
       "[400 rows x 3 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Purchased</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Purchased\n",
       "0            0\n",
       "1            0\n",
       "2            0\n",
       "3            0\n",
       "4            0\n",
       "..         ...\n",
       "395          1\n",
       "396          1\n",
       "397          1\n",
       "398          0\n",
       "399          1\n",
       "\n",
       "[400 rows x 1 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss = 144.84446:   0%|          | 13/5000 [00:00<00:38, 129.42it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss = 134.29922: 100%|██████████| 5000/5000 [00:34<00:00, 144.62it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 2.13206465,  0.66228359, -0.91532749])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the Logistic Regression model\n",
    "\n",
    "xs = features.values\n",
    "rescaled_xs = scale(xs)\n",
    "ys = target['Purchased'].values\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(rescaled_xs, ys, 0.33, seed=0)\n",
    "\n",
    "beta = fit(x_train=x_train, y_train=y_train, learning_rate = 0.001, max_iter = 5000)\n",
    "beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.13206465,  0.66228359, -0.91532749])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_vector = np.array(beta)\n",
    "beta_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1, -1,  0],\n",
       "       [ 0, -1,  1],\n",
       "       [-1,  0,  0],\n",
       "       [-1,  0,  1],\n",
       "       [-1,  0,  1],\n",
       "       [-1,  0,  0],\n",
       "       [ 0, -1,  0],\n",
       "       [ 0,  1,  1],\n",
       "       [ 0,  0,  1],\n",
       "       [ 1,  0,  0],\n",
       "       [ 0,  0,  1],\n",
       "       [-1, -1,  0],\n",
       "       [-1,  0,  0],\n",
       "       [ 0,  0,  1],\n",
       "       [ 0, -1,  1],\n",
       "       [ 0,  0,  1],\n",
       "       [ 0,  0,  1],\n",
       "       [-1, -1,  1],\n",
       "       [ 0,  0,  0],\n",
       "       [ 0,  0,  1],\n",
       "       [ 0,  0,  1],\n",
       "       [ 0,  0,  0],\n",
       "       [ 0,  0,  0],\n",
       "       [ 0,  0,  0],\n",
       "       [ 1,  1,  0],\n",
       "       [ 0,  0,  1],\n",
       "       [-1,  0,  0],\n",
       "       [ 0,  0,  0],\n",
       "       [ 0, -1,  0],\n",
       "       [ 2,  0,  1],\n",
       "       [ 0,  2,  0],\n",
       "       [ 1,  1,  0],\n",
       "       [ 0,  0,  0],\n",
       "       [ 0,  0,  1],\n",
       "       [-1,  0,  0],\n",
       "       [ 0,  0,  0],\n",
       "       [ 1,  1,  0],\n",
       "       [ 0,  0,  0],\n",
       "       [ 0,  0,  1],\n",
       "       [ 0,  0,  0],\n",
       "       [ 0,  0,  1],\n",
       "       [ 0,  0,  0],\n",
       "       [ 0,  0,  0],\n",
       "       [ 0,  0,  1],\n",
       "       [ 1,  0,  1],\n",
       "       [-1, -1,  1],\n",
       "       [ 0,  0,  1],\n",
       "       [ 0,  1,  0],\n",
       "       [ 0,  0,  1],\n",
       "       [-1,  0,  0],\n",
       "       [ 0,  0,  1],\n",
       "       [ 0,  0,  0],\n",
       "       [ 0,  1,  0],\n",
       "       [ 0,  0,  0],\n",
       "       [ 1, -1,  0],\n",
       "       [ 0,  0,  1],\n",
       "       [ 0,  2,  0],\n",
       "       [ 0,  0,  1],\n",
       "       [ 0,  1,  1],\n",
       "       [ 0,  0,  0],\n",
       "       [ 0,  0,  1],\n",
       "       [ 0,  0,  1],\n",
       "       [ 1,  0,  0],\n",
       "       [ 0,  0,  1],\n",
       "       [ 0,  1,  0],\n",
       "       [-1, -1,  1],\n",
       "       [ 1,  1,  0],\n",
       "       [ 0,  0,  1],\n",
       "       [ 0,  0,  1],\n",
       "       [ 1,  1,  0],\n",
       "       [ 0,  0,  1],\n",
       "       [ 1, -1,  1],\n",
       "       [ 0,  0,  0],\n",
       "       [ 1, -1,  0],\n",
       "       [ 0,  1,  1],\n",
       "       [ 0,  0,  0],\n",
       "       [ 0, -1,  1],\n",
       "       [-1,  0,  1],\n",
       "       [-1,  0,  1],\n",
       "       [ 0,  1,  1],\n",
       "       [ 0,  1,  1],\n",
       "       [ 2,  0,  1],\n",
       "       [ 0,  0,  0],\n",
       "       [ 0,  0,  1],\n",
       "       [ 0, -1,  0],\n",
       "       [-1,  0,  0],\n",
       "       [ 0,  0,  1],\n",
       "       [ 0,  0,  1],\n",
       "       [ 0,  0,  0],\n",
       "       [-1, -1,  1],\n",
       "       [ 0,  0,  1],\n",
       "       [ 0,  1,  0],\n",
       "       [-1,  0,  1],\n",
       "       [ 0,  1,  0],\n",
       "       [-1,  0,  0],\n",
       "       [ 2,  2,  1],\n",
       "       [ 0,  1,  0],\n",
       "       [ 0,  0,  0],\n",
       "       [ 0,  0,  0],\n",
       "       [ 0,  0,  1],\n",
       "       [-1, -1,  1],\n",
       "       [ 0,  0,  0],\n",
       "       [-1,  0,  0],\n",
       "       [ 0,  0,  0],\n",
       "       [ 0,  0,  0],\n",
       "       [ 0,  0,  1],\n",
       "       [ 0,  0,  0],\n",
       "       [ 0, -1,  1],\n",
       "       [ 0, -1,  0],\n",
       "       [ 0,  0,  0],\n",
       "       [-1,  0,  0],\n",
       "       [ 0,  0,  1],\n",
       "       [-1,  0,  1],\n",
       "       [ 1,  0,  1],\n",
       "       [ 0, -1,  1],\n",
       "       [ 0,  0,  1],\n",
       "       [-1, -1,  1],\n",
       "       [ 0,  0,  1],\n",
       "       [ 0,  0,  1],\n",
       "       [ 0,  0,  1],\n",
       "       [-1,  0,  1],\n",
       "       [ 0, -1,  1],\n",
       "       [ 1,  2,  0],\n",
       "       [ 0, -1,  1],\n",
       "       [ 0,  0,  0],\n",
       "       [-1,  0,  1],\n",
       "       [ 0,  2,  0],\n",
       "       [-1,  0,  1],\n",
       "       [ 2, -1,  0],\n",
       "       [ 0,  1,  0],\n",
       "       [ 0,  0,  1],\n",
       "       [ 0,  0,  1],\n",
       "       [ 0,  0,  0],\n",
       "       [ 1,  0,  0],\n",
       "       [ 1,  2,  1],\n",
       "       [ 0,  2,  0],\n",
       "       [ 0,  0,  1],\n",
       "       [ 1, -1,  0],\n",
       "       [ 0,  0,  0],\n",
       "       [ 0,  0,  0],\n",
       "       [ 0,  2,  1],\n",
       "       [ 0,  0,  0],\n",
       "       [-1,  0,  1],\n",
       "       [ 0,  0,  0],\n",
       "       [ 1,  0,  1],\n",
       "       [ 0,  0,  1],\n",
       "       [ 0,  0,  1],\n",
       "       [ 0,  0,  0],\n",
       "       [ 0,  0,  1],\n",
       "       [ 0,  0,  1],\n",
       "       [ 2,  0,  0],\n",
       "       [ 0,  1,  0],\n",
       "       [ 0,  1,  1],\n",
       "       [ 0,  0,  1],\n",
       "       [ 1,  1,  0],\n",
       "       [ 0,  0,  1],\n",
       "       [-1, -1,  0],\n",
       "       [-1, -1,  0],\n",
       "       [ 0,  0,  1],\n",
       "       [ 0,  0,  1],\n",
       "       [ 0,  0,  0],\n",
       "       [ 0,  0,  1],\n",
       "       [ 0,  0,  0],\n",
       "       [ 0,  0,  1],\n",
       "       [ 0,  0,  0],\n",
       "       [ 0,  1,  0],\n",
       "       [ 0,  0,  1],\n",
       "       [-1,  0,  1],\n",
       "       [ 0,  0,  0],\n",
       "       [ 0,  1,  0],\n",
       "       [-1,  0,  1],\n",
       "       [ 1, -1,  1],\n",
       "       [ 0,  0,  0],\n",
       "       [-1,  0,  0],\n",
       "       [ 0,  1,  0],\n",
       "       [ 1, -1,  1],\n",
       "       [ 0,  0,  1],\n",
       "       [ 0,  0,  1],\n",
       "       [ 0,  0,  0],\n",
       "       [ 0,  0,  1],\n",
       "       [-1, -1,  0],\n",
       "       [-1,  0,  1],\n",
       "       [ 0,  2,  1],\n",
       "       [ 0,  0,  0],\n",
       "       [ 0,  0,  0],\n",
       "       [ 1, -1,  0],\n",
       "       [-1, -1,  0],\n",
       "       [ 0,  0,  1],\n",
       "       [ 1,  2,  1],\n",
       "       [ 1,  0,  0],\n",
       "       [ 0,  0,  0],\n",
       "       [ 2,  0,  0],\n",
       "       [-1,  0,  0],\n",
       "       [ 0,  0,  0],\n",
       "       [ 2,  0,  0],\n",
       "       [ 0,  1,  1],\n",
       "       [ 0,  0,  0],\n",
       "       [ 0,  0,  1],\n",
       "       [ 2,  0,  0],\n",
       "       [-1,  0,  1],\n",
       "       [ 1, -1,  0],\n",
       "       [ 0,  1,  0],\n",
       "       [-1, -1,  0],\n",
       "       [ 0,  2,  1],\n",
       "       [ 1,  0,  1],\n",
       "       [-1,  0,  1],\n",
       "       [ 0,  0,  0],\n",
       "       [ 0,  0,  0],\n",
       "       [-1,  1,  0],\n",
       "       [-1, -1,  1],\n",
       "       [ 0, -1,  1],\n",
       "       [-1,  0,  0],\n",
       "       [ 1,  0,  1],\n",
       "       [ 0,  0,  0],\n",
       "       [ 0,  1,  0],\n",
       "       [ 1,  0,  1],\n",
       "       [ 0,  0,  1],\n",
       "       [-1,  0,  1],\n",
       "       [ 0, -1,  0],\n",
       "       [ 0,  0,  1],\n",
       "       [ 0,  0,  1],\n",
       "       [-1,  0,  0],\n",
       "       [ 0,  0,  1],\n",
       "       [ 0,  0,  1],\n",
       "       [ 1,  0,  0],\n",
       "       [ 1,  0,  0],\n",
       "       [ 0,  0,  1],\n",
       "       [ 0,  0,  1],\n",
       "       [-1, -1,  0],\n",
       "       [-1,  0,  0],\n",
       "       [ 1,  0,  0],\n",
       "       [-1,  0,  0],\n",
       "       [ 0, -1,  1],\n",
       "       [ 0, -1,  1],\n",
       "       [ 0,  0,  1],\n",
       "       [ 0, -1,  1],\n",
       "       [ 0, -1,  0],\n",
       "       [-1,  0,  1],\n",
       "       [-1, -1,  1],\n",
       "       [ 0,  2,  0],\n",
       "       [ 0,  0,  0],\n",
       "       [ 1,  2,  0],\n",
       "       [ 0,  0,  1],\n",
       "       [ 0,  0,  1],\n",
       "       [-1,  0,  0],\n",
       "       [ 0, -1,  0],\n",
       "       [ 0,  0,  0],\n",
       "       [ 0,  0,  1],\n",
       "       [ 0, -1,  0],\n",
       "       [ 0,  0,  0],\n",
       "       [ 2,  0,  1],\n",
       "       [ 0,  0,  0],\n",
       "       [ 0,  0,  0],\n",
       "       [-1,  0,  1],\n",
       "       [ 0, -1,  1],\n",
       "       [ 0, -1,  1],\n",
       "       [ 1,  0,  1],\n",
       "       [ 0, -1,  1],\n",
       "       [ 0,  0,  0],\n",
       "       [ 0,  0,  0],\n",
       "       [-1,  0,  1],\n",
       "       [ 0,  1,  0],\n",
       "       [ 0,  0,  0],\n",
       "       [ 0, -1,  1],\n",
       "       [ 0,  0,  0],\n",
       "       [ 0, -1,  0],\n",
       "       [ 1, -1,  0],\n",
       "       [ 0, -1,  1]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(x_train)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.79434824, -1.57761108, -2.13206465, -3.04739214, -3.04739214,\n",
       "       -2.13206465, -0.66228359, -0.2530439 , -0.91532749,  2.13206465,\n",
       "       -0.91532749, -2.79434824, -2.13206465, -0.91532749, -1.57761108,\n",
       "       -0.91532749, -0.91532749, -3.70967573,  0.        , -0.91532749,\n",
       "       -0.91532749,  0.        ,  0.        ,  0.        ,  2.79434824,\n",
       "       -0.91532749, -2.13206465,  0.        , -0.66228359,  3.34880181,\n",
       "        1.32456719,  2.79434824,  0.        , -0.91532749, -2.13206465,\n",
       "        0.        ,  2.79434824,  0.        , -0.91532749,  0.        ,\n",
       "       -0.91532749,  0.        ,  0.        , -0.91532749,  1.21673716,\n",
       "       -3.70967573, -0.91532749,  0.66228359, -0.91532749, -2.13206465,\n",
       "       -0.91532749,  0.        ,  0.66228359,  0.        ,  1.46978106,\n",
       "       -0.91532749,  1.32456719, -0.91532749, -0.2530439 ,  0.        ,\n",
       "       -0.91532749, -0.91532749,  2.13206465, -0.91532749,  0.66228359,\n",
       "       -3.70967573,  2.79434824, -0.91532749, -0.91532749,  2.79434824,\n",
       "       -0.91532749,  0.55445357,  0.        ,  1.46978106, -0.2530439 ,\n",
       "        0.        , -1.57761108, -3.04739214, -3.04739214, -0.2530439 ,\n",
       "       -0.2530439 ,  3.34880181,  0.        , -0.91532749, -0.66228359,\n",
       "       -2.13206465, -0.91532749, -0.91532749,  0.        , -3.70967573,\n",
       "       -0.91532749,  0.66228359, -3.04739214,  0.66228359, -2.13206465,\n",
       "        4.673369  ,  0.66228359,  0.        ,  0.        , -0.91532749,\n",
       "       -3.70967573,  0.        , -2.13206465,  0.        ,  0.        ,\n",
       "       -0.91532749,  0.        , -1.57761108, -0.66228359,  0.        ,\n",
       "       -2.13206465, -0.91532749, -3.04739214,  1.21673716, -1.57761108,\n",
       "       -0.91532749, -3.70967573, -0.91532749, -0.91532749, -0.91532749,\n",
       "       -3.04739214, -1.57761108,  3.45663184, -1.57761108,  0.        ,\n",
       "       -3.04739214,  1.32456719, -3.04739214,  3.60184571,  0.66228359,\n",
       "       -0.91532749, -0.91532749,  0.        ,  2.13206465,  2.54130435,\n",
       "        1.32456719, -0.91532749,  1.46978106,  0.        ,  0.        ,\n",
       "        0.40923969,  0.        , -3.04739214,  0.        ,  1.21673716,\n",
       "       -0.91532749, -0.91532749,  0.        , -0.91532749, -0.91532749,\n",
       "        4.2641293 ,  0.66228359, -0.2530439 , -0.91532749,  2.79434824,\n",
       "       -0.91532749, -2.79434824, -2.79434824, -0.91532749, -0.91532749,\n",
       "        0.        , -0.91532749,  0.        , -0.91532749,  0.        ,\n",
       "        0.66228359, -0.91532749, -3.04739214,  0.        ,  0.66228359,\n",
       "       -3.04739214,  0.55445357,  0.        , -2.13206465,  0.66228359,\n",
       "        0.55445357, -0.91532749, -0.91532749,  0.        , -0.91532749,\n",
       "       -2.79434824, -3.04739214,  0.40923969,  0.        ,  0.        ,\n",
       "        1.46978106, -2.79434824, -0.91532749,  2.54130435,  2.13206465,\n",
       "        0.        ,  4.2641293 , -2.13206465,  0.        ,  4.2641293 ,\n",
       "       -0.2530439 ,  0.        , -0.91532749,  4.2641293 , -3.04739214,\n",
       "        1.46978106,  0.66228359, -2.79434824,  0.40923969,  1.21673716,\n",
       "       -3.04739214,  0.        ,  0.        , -1.46978106, -3.70967573,\n",
       "       -1.57761108, -2.13206465,  1.21673716,  0.        ,  0.66228359,\n",
       "        1.21673716, -0.91532749, -3.04739214, -0.66228359, -0.91532749,\n",
       "       -0.91532749, -2.13206465, -0.91532749, -0.91532749,  2.13206465,\n",
       "        2.13206465, -0.91532749, -0.91532749, -2.79434824, -2.13206465,\n",
       "        2.13206465, -2.13206465, -1.57761108, -1.57761108, -0.91532749,\n",
       "       -1.57761108, -0.66228359, -3.04739214, -3.70967573,  1.32456719,\n",
       "        0.        ,  3.45663184, -0.91532749, -0.91532749, -2.13206465,\n",
       "       -0.66228359,  0.        , -0.91532749, -0.66228359,  0.        ,\n",
       "        3.34880181,  0.        ,  0.        , -3.04739214, -1.57761108,\n",
       "       -1.57761108,  1.21673716, -1.57761108,  0.        ,  0.        ,\n",
       "       -3.04739214,  0.66228359,  0.        , -1.57761108,  0.        ,\n",
       "       -0.66228359,  1.46978106, -1.57761108])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_scale_Y = X @ beta_vector\n",
    "log_scale_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05763035, 0.17113408, 0.10601915, 0.0453302 , 0.0453302 ,\n",
       "       0.10601915, 0.34022682, 0.43707443, 0.28591091, 0.89398085,\n",
       "       0.28591091, 0.05763035, 0.10601915, 0.28591091, 0.17113408,\n",
       "       0.28591091, 0.28591091, 0.02390025, 0.5       , 0.28591091,\n",
       "       0.28591091, 0.5       , 0.5       , 0.5       , 0.94236965,\n",
       "       0.28591091, 0.10601915, 0.5       , 0.34022682, 0.96606558,\n",
       "       0.78994056, 0.94236965, 0.5       , 0.28591091, 0.10601915,\n",
       "       0.5       , 0.94236965, 0.5       , 0.28591091, 0.5       ,\n",
       "       0.28591091, 0.5       , 0.5       , 0.28591091, 0.77148884,\n",
       "       0.02390025, 0.28591091, 0.65977318, 0.28591091, 0.10601915,\n",
       "       0.28591091, 0.5       , 0.65977318, 0.5       , 0.81302411,\n",
       "       0.28591091, 0.78994056, 0.28591091, 0.43707443, 0.5       ,\n",
       "       0.28591091, 0.28591091, 0.89398085, 0.28591091, 0.65977318,\n",
       "       0.02390025, 0.94236965, 0.28591091, 0.28591091, 0.94236965,\n",
       "       0.28591091, 0.63516823, 0.5       , 0.81302411, 0.43707443,\n",
       "       0.5       , 0.17113408, 0.0453302 , 0.0453302 , 0.43707443,\n",
       "       0.43707443, 0.96606558, 0.5       , 0.28591091, 0.34022682,\n",
       "       0.10601915, 0.28591091, 0.28591091, 0.5       , 0.02390025,\n",
       "       0.28591091, 0.65977318, 0.0453302 , 0.65977318, 0.10601915,\n",
       "       0.99074569, 0.65977318, 0.5       , 0.5       , 0.28591091,\n",
       "       0.02390025, 0.5       , 0.10601915, 0.5       , 0.5       ,\n",
       "       0.28591091, 0.5       , 0.17113408, 0.34022682, 0.5       ,\n",
       "       0.10601915, 0.28591091, 0.0453302 , 0.77148884, 0.17113408,\n",
       "       0.28591091, 0.02390025, 0.28591091, 0.28591091, 0.28591091,\n",
       "       0.0453302 , 0.17113408, 0.9694283 , 0.17113408, 0.5       ,\n",
       "       0.0453302 , 0.78994056, 0.0453302 , 0.97345075, 0.65977318,\n",
       "       0.28591091, 0.28591091, 0.5       , 0.89398085, 0.92698716,\n",
       "       0.78994056, 0.28591091, 0.81302411, 0.5       , 0.5       ,\n",
       "       0.60090556, 0.5       , 0.0453302 , 0.5       , 0.77148884,\n",
       "       0.28591091, 0.28591091, 0.5       , 0.28591091, 0.28591091,\n",
       "       0.98613095, 0.65977318, 0.43707443, 0.28591091, 0.94236965,\n",
       "       0.28591091, 0.05763035, 0.05763035, 0.28591091, 0.28591091,\n",
       "       0.5       , 0.28591091, 0.5       , 0.28591091, 0.5       ,\n",
       "       0.65977318, 0.28591091, 0.0453302 , 0.5       , 0.65977318,\n",
       "       0.0453302 , 0.63516823, 0.5       , 0.10601915, 0.65977318,\n",
       "       0.63516823, 0.28591091, 0.28591091, 0.5       , 0.28591091,\n",
       "       0.05763035, 0.0453302 , 0.60090556, 0.5       , 0.5       ,\n",
       "       0.81302411, 0.05763035, 0.28591091, 0.92698716, 0.89398085,\n",
       "       0.5       , 0.98613095, 0.10601915, 0.5       , 0.98613095,\n",
       "       0.43707443, 0.5       , 0.28591091, 0.98613095, 0.0453302 ,\n",
       "       0.81302411, 0.65977318, 0.05763035, 0.60090556, 0.77148884,\n",
       "       0.0453302 , 0.5       , 0.5       , 0.18697589, 0.02390025,\n",
       "       0.17113408, 0.10601915, 0.77148884, 0.5       , 0.65977318,\n",
       "       0.77148884, 0.28591091, 0.0453302 , 0.34022682, 0.28591091,\n",
       "       0.28591091, 0.10601915, 0.28591091, 0.28591091, 0.89398085,\n",
       "       0.89398085, 0.28591091, 0.28591091, 0.05763035, 0.10601915,\n",
       "       0.89398085, 0.10601915, 0.17113408, 0.17113408, 0.28591091,\n",
       "       0.17113408, 0.34022682, 0.0453302 , 0.02390025, 0.78994056,\n",
       "       0.5       , 0.9694283 , 0.28591091, 0.28591091, 0.10601915,\n",
       "       0.34022682, 0.5       , 0.28591091, 0.34022682, 0.5       ,\n",
       "       0.96606558, 0.5       , 0.5       , 0.0453302 , 0.17113408,\n",
       "       0.17113408, 0.77148884, 0.17113408, 0.5       , 0.5       ,\n",
       "       0.0453302 , 0.65977318, 0.5       , 0.17113408, 0.5       ,\n",
       "       0.34022682, 0.81302411, 0.17113408])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying Logistic function on to the log of odds scale to get the probabilty (prediction)\n",
    "Y = np.vectorize(logistic)(log_scale_Y)\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorizing(labelling) results with probs >= 0.5 as '1' *(Using threshold as 0.5)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_purchased = Y >= 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False,  True,  True,  True,  True,  True, False,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "       False, False, False, False, False,  True,  True,  True,  True,\n",
       "       False,  True,  True,  True,  True, False,  True,  True, False,\n",
       "       False, False,  True, False, False,  True,  True,  True,  True,\n",
       "        True,  True, False, False, False,  True,  True,  True,  True,\n",
       "        True,  True,  True, False, False, False,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True, False,  True,\n",
       "       False,  True, False, False,  True,  True,  True, False, False,\n",
       "        True, False,  True,  True,  True,  True,  True, False,  True,\n",
       "        True,  True,  True,  True,  True,  True, False, False, False,\n",
       "        True,  True, False,  True, False, False,  True, False,  True,\n",
       "       False, False,  True,  True,  True, False,  True, False,  True,\n",
       "        True, False,  True,  True, False,  True,  True, False,  True,\n",
       "        True,  True,  True,  True,  True,  True, False, False,  True,\n",
       "        True,  True,  True, False, False, False, False,  True, False,\n",
       "        True,  True,  True, False,  True,  True,  True,  True, False,\n",
       "        True,  True,  True,  True,  True,  True,  True, False,  True,\n",
       "       False,  True, False, False,  True,  True, False,  True,  True,\n",
       "        True, False,  True,  True,  True,  True,  True, False,  True,\n",
       "        True,  True,  True, False,  True,  True,  True,  True,  True,\n",
       "        True, False, False,  True, False,  True, False, False,  True,\n",
       "       False,  True,  True,  True,  True,  True, False,  True, False,\n",
       "       False,  True,  True,  True,  True,  True, False, False,  True,\n",
       "       False,  True,  True, False,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True, False, False,\n",
       "        True, False, False,  True,  True,  True, False,  True,  True,\n",
       "        True,  True,  True, False,  True, False,  True,  True, False,\n",
       "       False,  True, False, False,  True,  True,  True, False,  True,\n",
       "        True, False,  True, False, False,  True,  True])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correctly_predicted = predicted_purchased == y_train\n",
    "correctly_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.6753731343283582)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = correctly_predicted.sum() / len(correctly_predicted)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goodness of fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we predict paid account when ever proabability is greater than 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Confusion Matrix](https://miro.medium.com/v2/resize:fit:1400/1*pOtBHai4jFd-ujaNXPilRg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61.44776119402985, 0.5126050420168067, 0.6777777777777778)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming threshold to be probability >= 0.5 = True\n",
    "true_positives = false_positives = true_negatives = false_negatives = 0\n",
    "\n",
    "for x_i, y_i in zip(x_train, y_train):\n",
    "    prediction = logistic(np.dot(beta, x_i))\n",
    "    if y_i == 1 and prediction >= 0.5:\n",
    "        true_positives += 1  # paid and we predicted paid-TP\n",
    "    elif y_i == 1:\n",
    "        false_negatives += 1  # paid and predicted unpaid -FN\n",
    "    elif prediction >= 0.5:\n",
    "        false_positives += 1  # unpaid but predicted paid-FP\n",
    "    else:\n",
    "        true_negatives += 1  # unpaid and predicted unpaid-TN\n",
    "\n",
    "\n",
    "accuracy = true_positives + true_negatives / (\n",
    "    true_positives + false_positives + true_negatives + false_negatives\n",
    ")\n",
    "precision = true_positives / (true_positives + false_positives)\n",
    "recall = true_positives / (true_positives + false_negatives)\n",
    "\n",
    "accuracy, precision, recall"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
